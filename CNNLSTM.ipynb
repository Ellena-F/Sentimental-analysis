{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing required packages\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import callbacks\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import labeled and unlabled data\n",
    "df=pd.read_csv('labeled_data.csv',encoding='utf-8')\n",
    "test=pd.read_csv('unlabeled_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Had a good experience when my wife and I sat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On my first to Montreal with my gf we came her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our favorite places to go when it's col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The doctor was very nice, got in in a good amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nook is an immediate phoenix staple!  I ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Had a good experience when my wife and I sat a...\n",
       "1  On my first to Montreal with my gf we came her...\n",
       "2  One of our favorite places to go when it's col...\n",
       "3  The doctor was very nice, got in in a good amo...\n",
       "4  The Nook is an immediate phoenix staple!  I ca..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to remove punctuation\n",
    "import re\n",
    "def clean_phrase(phrase):\n",
    "\n",
    "    #Remove punctuation (with a regular expression) and convert to lower case\n",
    "    REPLACE= re.compile(\"[^a-zA-Z]\")\n",
    "    phrase = [REPLACE.sub(\" \", line.lower()) for line in phrase]\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftry=df\n",
    "testtry=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply function to data\n",
    "dftry.loc[:,'text']=clean_phrase(dftry.loc[:,'text'])\n",
    "testtry.loc[:,'text']=clean_phrase(testtry.loc[:,'text'])\n",
    "\n",
    "#convert 'text' to list\n",
    "clean_phrase = dftry.text.tolist()\n",
    "test_clean_phrase =testtry.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format data for analysis\n",
    "all_text=' /n '.join(clean_phrase)\n",
    "test_all_text=' /n '.join(test_clean_phrase)\n",
    "\n",
    "reviews=all_text.split(' /n ')\n",
    "all_text = ' '.join(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each word of the training dataset in the string to a list\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat same process as labeled data\n",
    "test_reviews=test_all_text.split(' /n ')\n",
    "test_all_text = ' '.join(test_reviews)\n",
    "\n",
    "test_words=test_all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 50000\n",
      "Test reviews: 600000\n"
     ]
    }
   ],
   "source": [
    "#chech number of reviews on both labeled and unlabled data (train and test data )\n",
    "print(\"Train reviews: {}\".format(len(reviews)))\n",
    "print(\"Test reviews: {}\".format(len(test_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store rating in array\n",
    "labels=dftry.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 5, ..., 1, 4, 1])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine words from labeled and unlabled data\n",
    "full_words = words + test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionaries that map the words in the vocabulary to integers. \n",
    "#Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(full_words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "#Build a dictionary that maps words to integers\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the words with integers. \n",
    "\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split( )])\n",
    "    \n",
    "test_reviews_ints = []\n",
    "for eachs in test_reviews:\n",
    "    test_reviews_ints.append([vocab_to_int[word] for word in eachs.split( )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 5\n",
      "Maximum review length: 1033\n"
     ]
    }
   ],
   "source": [
    "#check review lengths \n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check total no. of rows not having zero length reviews\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "len(non_zero_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zero length reviews\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "labels = np.array([labels[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 1033\n"
     ]
    }
   ],
   "source": [
    "#check again\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As maximum review length too many steps for RNN. Let's truncate to 12 steps. \n",
    "#For reviews shorter than 12 steps, we'll pad with 0s. For reviews longer than 12 steps,\n",
    "# we will truncate them to the first 12 characters.\n",
    "\n",
    "max_review_length = 12\n",
    "X_train = sequence.pad_sequences(reviews_ints, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(test_reviews_ints, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49995, 12)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177800\n"
     ]
    }
   ],
   "source": [
    "# check no of unique words in the corpus\n",
    "# Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "# this value will be passed to the embedding layer\n",
    "top_words = len(vocab_to_int) + 1\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding the labels\n",
    "y_train = np_utils.to_categorical(labels, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49995, 6)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chech data shape \n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Callbacks\n",
    "# ModelCheckpoints is used to save the model after every epoch\n",
    "# EarlyStopping is used to stop training when the validation loss has not improved after 2 epochs\n",
    "# Tensorboard is used tovisualize dynamic graphs of the training and test metrics\n",
    "cbks = [callbacks.ModelCheckpoint(filepath='./checkpoint_model.h5', monitor='val_loss', save_best_only=True),\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=2),callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 12, 32)            3917664   \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 12, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 3,974,574\n",
      "Trainable params: 3,974,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39996 samples, validate on 9999 samples\n",
      "Epoch 1/20\n",
      "39996/39996 [==============================] - 103s 3ms/step - loss: 1.3671 - accuracy: 0.3918 - val_loss: 1.2928 - val_accuracy: 0.4342\n",
      "Epoch 2/20\n",
      "39996/39996 [==============================] - 97s 2ms/step - loss: 1.1657 - accuracy: 0.5014 - val_loss: 1.2882 - val_accuracy: 0.4367\n",
      "Epoch 3/20\n",
      "39996/39996 [==============================] - 82s 2ms/step - loss: 1.0287 - accuracy: 0.5711 - val_loss: 1.3751 - val_accuracy: 0.4341\n",
      "Epoch 4/20\n",
      "39996/39996 [==============================] - 91s 2ms/step - loss: 0.8884 - accuracy: 0.6363 - val_loss: 1.4782 - val_accuracy: 0.4211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x66d211e80>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Model Architecture\n",
    "\n",
    "# embedding layer size\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(122427, embedding_vecor_length, input_length=max_review_length, dropout=0.2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# 1 layer of 100 units in the hidden layers of the LSTM cells\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train,validation_split=0.20, epochs=20,verbose=1, batch_size=32,callbacks=cbks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = load_model('checkpoint_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
